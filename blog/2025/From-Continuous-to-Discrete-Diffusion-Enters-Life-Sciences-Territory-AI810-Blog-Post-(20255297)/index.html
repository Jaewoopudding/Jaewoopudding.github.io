<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Continuous to Discrete: Diffusion Enters Life-Sciences Territory - AI810 Blog Post (20255297) | Jaewoo Lee </title> <meta name="author" content="Jaewoo Lee"> <meta name="description" content="Discrete diffusion models have recently leapt from continuous data such as images to discrete biological sequences, promising a new wave of goal-directed design for molecules, DNA, and proteins. This post surveys two ICLR-25 highlights: GenMol, which pairs a fragment-level masked-diffusion prior with light-weight test-time guidance to excel at drug-discovery tasks, and DRAKES, which casts diffusion fine-tuning as continuous-time reinforcement learning to optimise explicit rewards while staying close to the data prior. By contrasting their objectives, algorithms, and trade-offs, we expose a set of shared techniques—continuous-time masking, parallel token restoration, and KL-anchoring—that underpin both successes. We conclude that future progress hinges on aggressive test-time scaling: importing fast inference strategies from language modelling to cheaply explore vast design spaces before expensive wet-lab validation."> <meta name="keywords" content="Jaewoo Lee, Lee Jaewoo, Jaewoo"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%8B&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jaewoopudding.github.io/blog/2025/From-Continuous-to-Discrete-Diffusion-Enters-Life-Sciences-Territory-AI810-Blog-Post-(20255297)/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "From Continuous to Discrete: Diffusion Enters Life-Sciences Territory - AI810 Blog Post (20255297)",
            "description": "Discrete diffusion models have recently leapt from continuous data such as images to discrete biological sequences, promising a new wave of goal-directed design for molecules, DNA, and proteins. This post surveys two ICLR-25 highlights: GenMol, which pairs a fragment-level masked-diffusion prior with light-weight test-time guidance to excel at drug-discovery tasks, and DRAKES, which casts diffusion fine-tuning as continuous-time reinforcement learning to optimise explicit rewards while staying close to the data prior. By contrasting their objectives, algorithms, and trade-offs, we expose a set of shared techniques—continuous-time masking, parallel token restoration, and KL-anchoring—that underpin both successes. We conclude that future progress hinges on aggressive test-time scaling: importing fast inference strategies from language modelling to cheaply explore vast design spaces before expensive wet-lab validation.",
            "published": "June 01, 2025",
            "authors": [
              
              {
                "author": "Jaewoo Lee",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "KAIST",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jaewoo</span> Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>From Continuous to Discrete: Diffusion Enters Life-Sciences Territory - AI810 Blog Post (20255297)</h1> <p>Discrete diffusion models have recently leapt from continuous data such as images to discrete biological sequences, promising a new wave of goal-directed design for molecules, DNA, and proteins. This post surveys two ICLR-25 highlights: GenMol, which pairs a fragment-level masked-diffusion prior with light-weight test-time guidance to excel at drug-discovery tasks, and DRAKES, which casts diffusion fine-tuning as continuous-time reinforcement learning to optimise explicit rewards while staying close to the data prior. By contrasting their objectives, algorithms, and trade-offs, we expose a set of shared techniques—continuous-time masking, parallel token restoration, and KL-anchoring—that underpin both successes. We conclude that future progress hinges on aggressive test-time scaling: importing fast inference strategies from language modelling to cheaply explore vast design spaces before expensive wet-lab validation.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#why-discrete-diffusion-outshines-autoregression">Why Discrete Diffusion Outshines Autoregression</a> </div> <div> <a href="#but-likelihood-alone-won-t-get-you-there">…But Likelihood Alone Won’t Get You There</a> </div> <div> <a href="#a-glimpse-across-continuous-diffusion-and-llms">A Glimpse Across Continuous Diffusion and LLMs</a> </div> <div> <a href="#genmol-a-drug-discovery-generalist-with-discrete-diffusion">GenMol: A Drug Discovery Generalist with Discrete Diffusion</a> </div> <ul> <li> <a href="#motivation">Motivation</a> </li> <li> <a href="#methods">Methods</a> </li> <li> <a href="#experiments">Experiments</a> </li> </ul> <div> <a href="#fine-tuning-discrete-diffusion-models-via-reward-optimization-with-applications-to-dna-and-protein-design">Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to DNA and Protein Design</a> </div> <ul> <li> <a href="#motivation">Motivation</a> </li> <li> <a href="#methods">Methods</a> </li> <li> <a href="#experiments">Experiments</a> </li> </ul> <div> <a href="#shared-underlying-techniques">Shared Underlying Techniques</a> </div> <div> <a href="#different-design-choices-trade-offs">Different Design Choices &amp; Trade-offs</a> </div> <div> <a href="#conclusion-and-discussion">Conclusion and Discussion</a> </div> </nav> </d-contents> <h1 id="from-continuous-to-discrete-diffusion-enters-life-sciences-territory">From Continuous to Discrete: Diffusion Enters Life-Sciences Territory</h1> <p>Over the last few years, continuous-space diffusion models <d-cite key="ho2020denoising,song2020score"></d-cite> have delivered breakthroughs across multiple modalities—images <d-cite key="rombach2022high"></d-cite>, audio <d-cite key="liu2023audioldm"></d-cite>, even video <d-cite key="ho2022video"></d-cite> —by iteratively denoising Gaussian noise back into data.</p> <p>Diffusion is now entering its second act: extending that same paradigm to discrete data modalities <d-cite key="austin2021structured,sahoo2024simple"></d-cite>. This expansion holds the promise of bringing the same disruptive power to life-science data that continuous diffusion brought to computer vision.</p> <h2 id="why-discrete-diffusion-outshines-autoregression">Why Discrete Diffusion Outshines Autoregression</h2> <p>Discrete diffusion models possess several structural advantages over their main competitor, the autoregressive (AR) family:</p> <ul> <li>Parallel token updates. All positions are refined simultaneously, so inference scales sub-linearly with sequence length.</li> <li>Bidirectional context by construction. A sampler can denoise the tail positions first, then use that information to refine earlier tokens—impossible for strictly causal AR decoders.</li> <li>Reversible decisions. Because tokens are revisited through successive denoising steps, long-range constraints—chemical valence, Watson–Crick base pairing, ring closure—can be enforced post-hoc.</li> </ul> <p>Leveraging these properties, researchers are targeting three high-impact design challenges:</p> <table> <thead> <tr> <th>Application</th> <th>Modelling goal</th> <th>Typical reward signals</th> </tr> </thead> <tbody> <tr> <td><strong>Hit generation &amp; lead optimisation</strong></td> <td>Generate chemically valid graphs/strings that maximise binding affinity, ADMET safety, or docking scores.</td> <td>Docking ΔG, QED, synthesizability, Lipinski flags</td> </tr> <tr> <td><strong>DNA sequence design</strong></td> <td>Optimise ~200-bp enhancer sequences that drive high, cell-type-specific gene expression (e.g., HepG2).</td> <td>MPRA-based activity oracles, chromatin accessibility classifiers</td> </tr> <tr> <td><strong>Protein sequence design</strong></td> <td>Given a backbone conformation, produce amino-acid sequences that fold into it while improving thermodynamic stability (ΔΔG &lt; 0).</td> <td>Learned stability oracles, self-consistency RMSD</td> </tr> </tbody> </table> <h2 id="but-likelihood-alone-wont-get-you-there">…But Likelihood Alone Won’t Get You There</h2> <p>A naïve discrete diffusion model maximizes log-likelihood of the training data—excellent for reproducing what nature has already tried, but ill-suited for discovering sequences that satisfy new functional objectives. In practice the model:</p> <ol> <li>Ignores explicit objectives. Binding affinity, enzymatic turnover, or enhancer activity never appear in the likelihood.</li> <li>Overfits dataset bias. Regions of design space unrepresented in the data—often where the best molecules live—remain unexplored.</li> <li>Collapses under multi-objective trade-offs. Natural-looking but non-functional sequences dominate the sample pool.</li> </ol> <p>To see how the discrete diffusion community is attacking these limitations, we will examine two recent ICLR-25 papers—GenMol and DRAKES—that push discrete diffusion into goal-directed generation for molecules and DNA.</p> <table> <thead> <tr> <th>Model</th> <th>Diffusion backbone</th> <th>Objective-alignment strategy</th> <th>Key tricks</th> </tr> </thead> <tbody> <tr> <td><strong>GenMol</strong></td> <td>Masked Discrete Language Model (MDLM) style diffusion</td> <td> <em>Test-time</em> remasking &amp; denoising guided by an internal scoring head</td> <td>Fragment remasking, Molecular-Context Guidance (MCG)</td> </tr> <tr> <td><strong>DRAKES</strong></td> <td>Gumbel-softmax CTMC diffusion</td> <td> <em>Training-time</em> reward finetuning via Direct Diffusion Policy Optimization (DDPO)</td> <td>Reward-weighted ELBO, KL control</td> </tr> </tbody> </table> <p>These papers illustrate complementary philosophies:</p> <p>GenMol keeps the base model frozen and injects guidance during inference, amortising nothing. DRAKES invests computation during finetuning to bake the reward into the generator, enabling cheaper deployment.</p> <h2 id="a-glimpse-across-continuous-diffusion-and-llms">A Glimpse Across Continuous Diffusion and LLMs</h2> <p>The tension between likelihood training and task rewards is hardly unique to discrete diffusion. In both continuous diffusion and large language models (LLMs), researchers have explored three broad families of solutions:</p> <table> <thead> <tr> <th>Family</th> <th>Core idea</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td> <strong>RL finetuning</strong> <d-cite key="black2023training,ouyang2022training"></d-cite> </td> <td>Treat sampling as a Markov Decision Process; optimise an external reward with policy-gradient or Q-learning variants.</td> <td>Universally applicable, strong theoretical footing</td> <td>Requires proxy reward model; high variance; mode collapse risk</td> </tr> <tr> <td> <strong>Direct Policy Optimization</strong> <d-cite key="rafailov2023direct,wallace2024diffusion"></d-cite> </td> <td>Minimise a KL-regularised contrastive loss that pushes up the log-probability of high-reward (or preferred) samples against low-reward ones in an offline setting.</td> <td>Stable, simpler than vanilla RL, no reward proxy model</td> <td>Zero exploration due to offline property</td> </tr> <tr> <td> <strong>Test-time guidance</strong> <d-cite key="snell2024scaling,ma2025inference"></d-cite> </td> <td>Modify the sampler (classifier guidance, score distillation, speculative decoding) without touching base weights.</td> <td>No retraining; amortised cost only at inference</td> <td>Extra decoding passes; limited by oracle accuracy</td> </tr> </tbody> </table> <p>In the sections that follow we dive into GenMol and DRAKES, dissect how each integrates rewards into discrete diffusion, and draw lessons for building the next generation of goal-oriented generative models in life sciences.</p> <h1 id="genmol-a-drug-discovery-generalist-with-discrete-diffusion">GenMol: A Drug Discovery Generalist with Discrete Diffusion</h1> <h2 id="motivation">Motivation</h2> <p>Fragment-Based Drug Design (FBDD) has become a workhorse in both pharma and biotech because it lets chemists <strong>grow or link small, experimentally validated fragments</strong> into potent leads while keeping synthetic routes short.<br> Yet most generative models still operate at the <strong>atom–bond graph</strong> level or on canonical SMILES strings. This brings three pain points:</p> <ol> <li> <strong>Semantic mismatch.</strong> Atom-level tokens ignore medicinal-chemistry priors—ring systems, hetero-substituted aromatics, privileged scaffolds—that chemists actually manipulate.</li> <li> <strong>Combinatorial drag.</strong> Autoregressive (AR) SMILES generators explore chemical space token-by-token; fragment substitutions that appear near the <em>end</em> of the string are never “seen” when earlier tokens are chosen.</li> <li> <strong>Task fragmentation.</strong> Separate models or costly re-training are typically required for de-novo enumeration, scaffold decoration, fragment linking, and reward-guided optimisation.</li> </ol> <p>GenMol tackles these issues head-on by unifying <strong>masked discrete diffusion</strong> with the <em>Structure-Aware Fragment Encoding</em> (<strong>SAFE</strong>)<d-cite key="noutahi2024gotta"></d-cite> representation:</p> <ul> <li> <strong>SAFE Strings.</strong> Each token is a <em>chemically meaningful fragment</em> whose identity does <strong>not</strong> depend on permutation or attachment order, eliminating SMILES grammar headaches and yielding a vocabulary aligned with medicinal-chemistry intuition.</li> <li> <strong>Bidirectional masked diffusion.</strong> Unlike AR decoders, GenMol refines <strong>all</strong> fragment tokens in parallel. The sampler can therefore denoise the tail fragments first—honouring global constraints such as valence balance or docking hotspots—and then revisit earlier fragments to ensure compatibility.</li> <li> <strong>Single checkpoint, multi-task.</strong> A single GenMol model, trained once on a large SAFE corpus, can be steered at inference time—via Molecular-Context Guidance or external rewards—across the full fragment-based design workflow.</li> </ul> <p>In short, GenMol is motivated by the need for a <strong>chemistry-aligned, task-agnostic generator</strong> that preserves the empirical strengths of FBDD while exploiting the global context and parallelism that discrete diffusion uniquely offers.</p> <hr> <h2 id="methods">Methods</h2> <h3 id="1-masked-discrete-diffusion">1 Masked discrete diffusion</h3> <p>GenMol Training objective minimizes the non-equilibrium ELBO from MDLM <d-cite key="sahoo2024simple"></d-cite>:</p> \[\mathcal{L}_{\text{NELBO}} = \mathbb{E}_{q}\!\Bigg[ \int_{0}^{1} \frac{\alpha_t'}{1-\alpha_t} \sum_{l} \log\!\big\langle x_{\theta,l}(z_t,t),\,x_l \big\rangle \, dt \Bigg].\] <p>Here (z<em>t) is a partially masked SAFE sequence at continuous time (t); (\alpha_t) is the masking schedule.<br> During sampling, _all</em> masked indices are updated in parallel:</p> \[p_\theta^{(l)}\!\bigl(z_s \mid z_t\bigr) = \operatorname{Cat}\!\Biggl[ 1-\alpha_s\,m + (\alpha_s-\alpha_t)\; \frac{ \exp\!\bigl(\tfrac{1}{\tau}\, \log x_{\theta,l}(z_t,t)\bigr) }{ \displaystyle \sum_{j} \exp\!\bigl(\tfrac{1}{\tau}\, \log x_{\theta,l}^{(j)}(z_t,t)\bigr) } \Biggr],\] <p>where (\tau) is the soft-max temperature.</p> <h3 id="2molecular-context-guidance-mcg">2 Molecular-Context Guidance (MCG)</h3> <p>MCG borrows the spirit of classifier-free guidance in image diffusion but removes the need for an <strong>external</strong> property predictor.<br> Instead, GenMol trains <strong>two views of the <em>same</em> denoiser</strong> in a multi-task fashion:</p> <table> <thead> <tr> <th>View</th> <th>Conditioning tokens</th> <th>Role</th> </tr> </thead> <tbody> <tr> <td> <strong>Context-aware head</strong> <code class="language-plaintext highlighter-rouge">$$\(x\_\theta^{\text{ctx}}\)$$</code> </td> <td>full SAFE string <strong>plus</strong> an auxiliary <em>context vector</em> <code class="language-plaintext highlighter-rouge">$$\\(c\)$$</code> (e.g. fragment bag-of-words, docking hotspot mask, scaffold type)</td> <td>Learns to reconstruct fragments that are globally consistent with <code class="language-plaintext highlighter-rouge">$$\(c\)$$</code>.</td> </tr> <tr> <td> <strong>Context-free head</strong> <code class="language-plaintext highlighter-rouge">$$\(x\_\theta\)$$</code> </td> <td>SAFE string <strong>only</strong>, drop the context vector (replace with <code class="language-plaintext highlighter-rouge">\&lt;no_ctx\&gt;</code> token)</td> <td>Learns the unconditional data distribution.</td> </tr> </tbody> </table> <p>During <strong>training</strong> the two heads share all transformer layers except the final projection, so extra parameters are negligible.<br> At <strong>sampling</strong> time we blend the two logits:</p> <div align="center"> \[ \log x*\theta^{(w)}(z_t,t,c) \;=\; w\;\log x*\theta(z*t,t)\;+\; (1-w)\;\log x*\theta^{\text{ctx}}(z_t,t,c), \tag{4} \] </div> <ul> <li> <code class="language-plaintext highlighter-rouge">$$\(w\in[0,1]\)$$</code> is the <strong>guidance weight</strong>.</li> <li> <code class="language-plaintext highlighter-rouge">$$\(w\!\downarrow\)$$</code> → pure, unbiased samples; <code class="language-plaintext highlighter-rouge">$$\(w\!\uparrow\)$$</code> → context-faithful but potentially lower diversity.</li> <li>Because both heads live in the same network, the blend costs <strong>one</strong> forward pass—no extra GPU memory, no back-prop, no separate classifier.</li> </ul> <p><strong>Why it works.</strong><br> Intuitively, <code class="language-plaintext highlighter-rouge">$$\(\log x*\theta^{\text{ctx}}\)$$</code> raises probability mass on tokens that fit the desired global structure, while <code class="language-plaintext highlighter-rouge">$$\(\log x*\theta\)$$</code> provides a diversity-preserving prior. Their linear mix shifts the reverse diffusion drift toward molecules that honour the context without collapsing onto a single mode.</p> <h3 id="3confidence-based-sampling">3 Confidence-Based Sampling</h3> <p>Standard diffusion would resample <strong>every</strong> mask at every step, frequently overwriting good predictions. GenMol instead adopts <strong>confidence sampling</strong> <d-cite key="chang2022maskgit"></d-cite> with three hyper-parameters <code class="language-plaintext highlighter-rouge">$$\((N,\tau,r)\)$$</code>. For each still-masked position <code class="language-plaintext highlighter-rouge">$$\(l\)$$</code> at reverse-time <code class="language-plaintext highlighter-rouge">$$\(t\)$$</code>:</p> <ol> <li> <p><strong>Predict a token.</strong><br> Apply the soft-max with temperature <code class="language-plaintext highlighter-rouge">$$\(\tau\)$$</code> to the mixed logits from Eq. (4) and sample an index <code class="language-plaintext highlighter-rouge">$$\(i^\ast\)$$</code> for each still-masked position <code class="language-plaintext highlighter-rouge">$$\(l\)$$</code>.</p> </li> <li> <p><strong>Compute a confidence score</strong> that blends model certainty with controllable exploration:</p> \[c_t^{(l)} \;=\; \log p_{\theta,i^\ast}^{(l)} \;+\; r\,t\,\varepsilon, \qquad \varepsilon \sim \text{Gumbel}(0,1) \tag{8}\] <p>Here (p_{\theta,i^\ast}^{(l)}) is the model’s probability for the sampled category at time step <code class="language-plaintext highlighter-rouge">$$\(t\)$$</code>,<br> while the additive Gumbel noise (\varepsilon) is <strong>scaled by (r\,t)</strong>—large and exploratory in early iterations, then cooling off as <code class="language-plaintext highlighter-rouge">$$\(t \!\downarrow\)$$</code> to make late steps increasingly deterministic.</p> </li> <li> <p><strong>Freeze the confident tokens.</strong><br> Rank all still-masked positions by their <code class="language-plaintext highlighter-rouge">$$\(c*t^{(l)}\)$$</code> scores and permanently unmask the top <code class="language-plaintext highlighter-rouge">$$\(N\)$$</code>; the remainder stay masked for the next reverse-diffusion step. Effectively, GenMol makes a _parallel* guess for every fragment, then “commits” only the most confident ones.<br> The procedure repeats until no masks remain, yielding:</p> </li> </ol> <ul> <li> <strong>Quality–speed trade-off.</strong><br> Larger (N) or smaller (\tau) reduces steps (faster) but can hurt quality; higher randomness (r) broadens exploration.<br> Appendix B shows that <code class="language-plaintext highlighter-rouge">$$\(N\!=\!1,\;\tau\!=\!0.5,\;r\!=\!0.5\)$$</code> attains the best validity/quality balance, while (N!=!3) triples speed with only moderate quality loss. :contentReference[oaicite:5]{index=5}</li> <li> <strong>Context sensitivity.</strong><br> Because only high-confidence positions freeze, later denoising rounds can adapt earlier uncertain fragments to satisfy long-range chemistry constraints, a capability AR models lack.</li> </ul> <hr> <h2 id="experiments">Experiments</h2> <p>GenMol is trained <strong>once</strong> on the SAFE corpus and then reused <em>unchanged</em> across four evaluations.</p> <ul> <li> <p><strong>De-novo generation (ZINC-250k).</strong><br> With (N=1,\;\tau=0.5,\;r=0.5), GenMol achieves 100 % validity, 99.7 % uniqueness, and pushes the “quality” metric (valid ∧ drug-like ∧ synthesizable) to <strong>84.6 %</strong>, far above SAFE-GPT’s 54.7 %, while sampling faster.</p> </li> <li> <p><strong>Fragment-constrained generation (10 FDA drugs).</strong><br> Across five constraint tasks, GenMol lifts success rates by 5–16 pp and maintains ≥ 96 % validity, outperforming SAFE-GPT on every metric–task pair.</p> </li> <li> <p><strong>Goal-directed hit generation (23 PMO targets).</strong><br> Fragment remasking + MCG delivers the top-10 AUC on 19 / 23 targets and the highest summed AUC across all baselines (REINVENT, Graph-GA, BO, etc.).</p> </li> <li> <p><strong>Lead optimisation (docking vs. PARP1, BRAF, JAK2, …).</strong><br> Starting from experimental seeds, GenMol lowers docking energies by up to <strong>3 kcal·mol⁻¹</strong> relative to seeds and surpasses GA-based optimisers on four of six proteins.</p> </li> </ul> <p>All optimisation runs execute on <strong>a single GPU</strong> with no additional fine-tuning, underscoring the practicality of masked discrete diffusion paired with fragment-level exploration.</p> <h1 id="fine-tuning-discrete-diffusion-models-via-reward-optimization-with-applications-to-dna-and-protein-design">Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to DNA and Protein Design</h1> <h2 id="motivation-1">Motivation</h2> <p>State-of-the-art <strong>masked <em>discrete diffusion</em></strong> models already learn an excellent <em>prior</em> over biological sequences (DNA enhancers, proteins, even text) by running a <strong>continuous-time Markov chain (CTMC)</strong> <d-cite key="campbell2022continuous"></d-cite>that randomly <em>masks</em> tokens and then denoises them.<br> Yet real design tasks demand <strong>pushing samples toward explicit rewards</strong>—expression level, folding stability, toxicity control—<em>without destroying that powerful diffusion prior</em>. Naïve reward-only fine-tuning breaks down: the model “reward-hacks”, drifts OOD, and loses validity.</p> <p><strong>DRAKES</strong> tackles this by casting <em>fine-tuning of a discrete-diffusion model itself</em> as a <strong>continuous-time RL problem</strong> that <strong>maximises reward <em>and</em> keeps the generator close (in KL) to the original diffusion kernel</strong>. The outcome: sequences that stay “natural-like” while reaching much higher task scores.</p> <hr> <h2 id="methods-1">Methods</h2> <h3 id="1continuous-time-rl-on-a-discrete-diffusion-generator">1 Continuous-time RL on a Discrete-Diffusion Generator</h3> <p>Start from a pre-trained <strong>masked discrete-diffusion CTMC</strong> with transition rates (Q<em>{\text{pre}}(t)).<br> We search for new rates (Q</em>\theta(t)) that solve</p> <p>[ \max<em>\theta\; \underbrace{\mathbb{E}</em>{x<em>{0:T}\sim P</em>\theta}\big[r(x_T)\big]}<em>{\text{task reward}}\; -\; \alpha\, \underbrace{\mathbb{E}</em>{x<em>{0:T}\sim P</em>\theta}!!\int<em>0^{T} \mathrm{KL}\bigl(Q</em>\theta \,\Vert\, Q<em>{\text{pre}}\bigr)\,dt}</em>{\text{stay near the diffusion prior}}. ]</p> <p><em>Because the base generator is itself a CTMC discrete diffusion, this KL is computed <strong>at every infinitesimal denoising step</strong>, directly regularising the entire diffusion path.</em><br> At optimum the terminal distribution is the <strong>exponentially tilted diffusion prior</strong></p> <p>[ p^\star<em>T(x)\;\propto\; e^{r(x)/\alpha}\,p</em>{\text{pre}}(x), ]</p> <p>so the original diffusion manifold is preserved while reward is boosted.</p> <h3 id="2direct-reward-backpropagation-with-gumbel-softmax-drakes">2 Direct Reward bAcKpropagation with gumbEl Softmax (DRAKES)</h3> <table> <thead> <tr> <th>Stage</th> <th>Discrete-diffusion angle</th> <th>Details</th> </tr> </thead> <tbody> <tr> <td><strong>Sampling</strong></td> <td>Make CTMC <em>diffusion</em> trajectories differentiable</td> <td>Replace every discrete fragment-token hop with a <strong>Gumbel–Softmax</strong> relaxation, so gradients propagate through the diffusion steps.</td> </tr> <tr> <td><strong>Optimisation</strong></td> <td>Gradient ascent on the RL objective</td> <td>Compute minibatch estimates, back-prop through the <em>relaxed diffusion path</em>, and update (\theta) with Adam. A straight-through trick on the final (hard) denoise step sharpens oracle alignment.</td> </tr> </tbody> </table> <blockquote> <p>All learning happens <em>inside</em> the diffusion model—no separate policy network or value critic is introduced.</p> </blockquote> <h3 id="3theory-links-to-diffusion">3 Theory Links to Diffusion</h3> <ul> <li>The optimal rates satisfy<br> (Q^\star<em>{x,y}(t)=Q^{\text{pre}}</em>{x,y}(t)\exp!\bigl((V_t(y)-V_t(x))/\alpha\bigr)),<br> mirroring a <strong>score-matching update</strong> inside the diffusion kernel.</li> <li>Classic <strong>classifier guidance for discrete diffusion</strong> appears as a <em>Doob transform</em> special case; unlike DRAKES it requires no training but omits KL regularisation and underperforms on proteins.</li> </ul> <hr> <h2 id="experiments-1">Experiments</h2> <p><strong>DRAKES is fine-tuned on a task-specific reward model and evaluated—without further architectural changes—across three very different domains.</strong></p> <ul> <li> <p><strong>DNA enhancer design.</strong><br> RL fine-tuning boosts the predicted transcriptional <em>activity</em> of generated 200-bp sequences by <strong>≈ 13 %</strong> relative to the strongest baseline, <em>while simultaneously</em> increasing motif fidelity and 3-mer realism.<br> → Shows that the KL-regularised CTMC keeps samples “natural-like” even under strong reward pressure.</p> </li> <li> <p><strong>Protein inverse folding (Megascale stability set).</strong><br> The proportion of designed sequences that both <em>fold</em> and remain <em>thermodynamically stable</em> climbs to <strong>78.6 %</strong>, compared with <strong>≈ 64 %</strong> for SMC or TDS guidance.<br> → Demonstrates that end-to-end back-prop through discrete hops can out-perform search-based guidance.</p> </li> <li> <p><strong>Toxicity-controlled text generation.</strong><br> Achieves the best mean and median <em>toxicity scores</em> among all contenders (pre-trained diffusion, classifier-guided diffusion, SMC, TDS).<br> → Confirms that the method generalises beyond biological tokens to natural-language tasks.</p> </li> </ul> <p><em>Ablation studies</em> verify that the trajectory-level KL term is essential for preventing reward hacking, and that the Gumbel-Softmax temperature schedule remains robust over a wide hyper-parameter range.</p> <hr> <h1 id="shared-underlying-techniques">Shared Underlying Techniques</h1> <table> <thead> <tr> <th>Core element</th> <th><strong>GenMol</strong></th> <th><strong>DRAKES</strong></th> <th>Shared benefit</th> </tr> </thead> <tbody> <tr> <td><strong>Forward (noise) step</strong></td> <td>Randomly <strong>mask</strong> SAFE-fragments → continuous-time CTMC</td> <td>Randomly <strong>mask</strong> sequence tokens → CTMC</td> <td>Equivalent to a continuous-time BERT-MLM objective</td> </tr> <tr> <td><strong>Reverse (denoising) step</strong></td> <td>Transformer predicts <em>all</em> fragments <strong>in parallel</strong> (non-autoregressive)</td> <td>Same parallel token restoration</td> <td>Length flexibility, fast sampling, order invariance</td> </tr> </tbody> </table> <blockquote> <p><strong>In short:</strong> Both papers extend the BERT masking idea to continuous time, yielding <em>masked discrete diffusion</em> models that inherit parallel decoding and amenability to gradient guidance.</p> </blockquote> <hr> <h1 id="different-design-choices--trade-offs">Different Design Choices &amp; Trade-offs</h1> <table> <thead> <tr> <th>Perspective</th> <th> <strong>GenMol</strong> <em>(fragment-based drug design)</em> </th> <th> <strong>DRAKES</strong> <em>(biology &amp; text reward tuning)</em> </th> </tr> </thead> <tbody> <tr> <td><strong>Primary goal</strong></td> <td>“<strong>One-shot pre-train, multi-task zero-shot</strong>” exploration (de-novo, fragment linking, scaffold decoration, etc.)</td> <td> <strong>Fine-tune for explicit rewards</strong> (enhancer activity, protein stability, toxicity control)</td> </tr> <tr> <td><strong>Guidance method</strong></td> <td> <em>Molecular-Context Guidance</em> (logit blending) + reward-free fragment remasking</td> <td>Gumbel-Softmax relaxation enables <strong>direct reward-gradient back-prop</strong> </td> </tr> <tr> <td><strong>Supervision signal</strong></td> <td>Large SAFE corpus only; <em>no</em> reward model required</td> <td>Requires an <strong>external reward/oracle</strong>; RL fine-tuning with trajectory-level KL</td> </tr> <tr> <td><strong>Naturalness guarantee</strong></td> <td>Uses the original fragment prior; chemically meaningful SAFE tokens ensure synthesizability</td> <td>Adjustable α-weighted KL keeps samples near the pretrained prior</td> </tr> <tr> <td><strong>Representation unit</strong></td> <td> <strong>Chemistry fragments (SAFE)</strong> → embeds medicinal-chemistry priors</td> <td>Generic <strong>tokens</strong> (bases / amino acids / words) → domain-agnostic</td> </tr> <tr> <td><strong>Sampling strategy</strong></td> <td>Confidence-freezing + fragment remasking → rapid local edits</td> <td>Standard CTMC sampling with reward gradients influencing every hop</td> </tr> <tr> <td><strong>Compute cost</strong></td> <td>Single pre-training run; inference light (1 GPU)</td> <td>Extra fine-tuning epochs with Gumbel-Softmax back-prop (heavier GPU demand)</td> </tr> </tbody> </table> <h1 id="conclusion-and-discussion">Conclusion and Discussion</h1> <p>Discrete diffusion is rapidly emerging as a unifying framework for goal-oriented generation in the life sciences. GenMol shows that a single masked-diffusion prior over chemistry fragments can serve as a versatile, medicinal-chemistry-aligned generator, while DRAKES demonstrates that the same mathematical engine can be reward-tuned—without sacrificing validity—to meet demanding objectives in DNA, proteins, and even natural-language sequences.</p> <p>However, applying diffusion to real biological design problems is still in its infancy, and much work lies ahead. My conjecture on this research area is that test-time scaling will become critically important for life science. The domains we care about—drug discovery, synthetic biology, protein engineering—often require extremely expensive evaluation protocols (wet-lab assays, in-vitro and in-vivo studies). Inference-time computation is cheap by comparison, but it must become cheaper still so we can explore vast design spaces before committing to physical validation. A promising path is to import test-time computation strategies proven indispensable in modern language modelling—speculative decoding, draft-and-refine loops, classifier-free or hybrid guidance—and adapt them to discrete diffusion in the scientific setting.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-06-01-From-Continuous-to-Discrete:-Diffusion-Enters-Life-Sciences-Territory-AI810-Blog-Post-(20255297).bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jaewoo Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>